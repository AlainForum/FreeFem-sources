//ff-mpirun  -np  4 diffusion-3d-intermediate.edp -glut ffglut -raspart -ffddm_schwarz_method ras -ffddm_schwarz_coarse_correction BNN -ffddm_geneo_nu 10 -global 20 -ffddm_plot

// If you have openmpi you may need to add the option --oversubscribe to allow more processes than the number of cores available on your computer

macro dimension 3// EOM            // 2D or 3D

include "ffddm.idp"

macro def(i)i// EOM                         // scalar field definition
macro init(i)i// EOM                        // scalar field initialization
macro grad(u)[dx(u), dy(u), dz(u)]// EOM    // three-dimensional gradient
func Pk = P1;                               // finite element space

int[int] LL = [2,2, 1,2, 2,2];
meshN ThGlobal = cube(getARGV("-global", 10), getARGV("-global", 10), getARGV("-global", 10),
    [x, y, z], label = LL);      // global mesh


macro Varf(varfName, meshName, PhName)
    varf varfName(u,v) = intN(meshName)(grad(u)' * grad(v)) + intN(meshName)(v) + on(1, u = 1.0); // EOM
    

    
// Domain decomposition
ffddmbuildDmesh(Lapmesh,ThGlobal,mpiCommWorld)
savemesh(LapmeshThi,"localTh"+mpirank+".mesh");
ffddmbuildDfespace(LapFE,Lapmesh,real,def,init,P1)
// alternative faire 28 et 30 en meme temps avec  ffddmbuild(Lap,ThGlobal,def,init,P1,mpiCommWorld);
// pous Lap Lap 
// principe de base l'utilisateur ne maitrise pas qui est qui => solution tout appeler Lap;
ffddmsetupOperator(Lap,LapFE,Varf)
	//distributed matrix vector product
LapFEVhi uux=1.;
LapA(uux[]);

real volume , sommevolume , volumeloc , volumelocpondere;
volumelocpondere = intN(LapmeshThi)(LapmeshDP1[mpirank]);
volumeloc = intN(LapmeshThi)(1.);

// volume du domaine de calcul
cout << "volume local du sous domaine " << mpirank << " : " << volumeloc << "\n" << endl;
cout << "volume local pondere " << mpirank << " : "  << volumelocpondere  << "\n" << endl;

mpiAllReduce( volumelocpondere , volume   , mpiCommWorld , mpiSUM);
mpiAllReduce( volumeloc , sommevolume   , mpiCommWorld , mpiSUM);
if (mpirank == 0) cout << "Somme des volumes locaux :  " << sommevolume <<   "\n" << endl;

if (mpirank == 0) cout << "Somme des volumes locaux ponderes :  " << volume <<  "\n" << endl;

// Distributed Direct and domain decomposition solves
real[int] rhs(LapFEVhi.ndof);//rhs(1) works as well 
ffddmbuildrhs(Lap,Varf,rhs)
LapFEVhi def(u) , def(udirectsolver);

//Direct solve
if (mpirank == 0) cout << endl << "Lap: Direct solver (MUMPS) :" << endl;//direct parallel solver 
udirectsolver[] = Lapdirectsolve(rhs);
Lapwritesummary//process 0 prints convergence history
ffddmplot(Lap,udirectsolver, "Lap Global solution with direct solver");

// Two-level Schwarz solve
if (mpirank == 0) cout << endl << "Lap: RAS + GENEO :" << endl; //second level method with a GenEO coarse space
ffddmsetupPrecond(Lap,Varf)
ffddmgeneosetup(Lap,Varf)

real[int] x0(LapFEVhi.ndof);
x0 = 0;
u[] = LapfGMRES(x0, rhs, 1.e-6, 200, "right");
Lapwritesummary//process 0 prints convergence history
ffddmplot(Lap,u, "Lap Global solution with fGMRES");

// Visualization and computation of relative error w.r.t. direct solver 
LapFEVhi error;
error[] = u[]-udirectsolver[];
real solnorm2 = LapFEscalprod(u[],u[]) ,  errnorm2 = LapFEscalprod(error[],error[])  ;
if(mpirank == 0) cout << endl << "Relative error w.r.t. to direct solver : " <<  sqrt(errnorm2/solnorm2) << "\n" << endl;
ffddmplot(Lap,error, "Difference between direct and ddm solve");

